# configs/models_config.yaml

models:
  # 1. 本地 VLLM 部署的模型 (使用 OpenAI 兼容接口)
  qwen3-4b-local:
    provider: "openai_compatible"
    display_name: "Qwen3-4B-Instruct (本地)"
    api_base: "http://localhost:8000/v1"
    api_key: "not-used"
    
    model_name: "/home/lee/.cache/modelscope/hub/models/Qwen/Qwen3-4B-Instruct-2507" 
    
    parameters:
      temperature: 0.7
      max_tokens: 4096

  llama3-8b-vllm:
    provider: "openai_compatible"
    display_name: "Llama-3-8B (Local VLLM)"
    api_base: "http://localhost:8000/v1"
    api_key: "not-used"
    model_name: "meta-llama/Meta-Llama-3-8B-Instruct" 
    parameters:
      temperature: 0.7
      max_tokens: 4096

  # 2. 在线 OpenAI API 模型 (示例)
  # gpt-4o:
  #   provider: "openai_compatible"
  #   display_name: "GPT-4o (OpenAI API)"
  #   api_base: "https://api.openai.com/v1"
  #   api_key: "ENV:OPENAI_API_KEY" # 建议从环境变量读取
  #   model_name: "gpt-4o"
  #   parameters:
  #     temperature: 0.8
  #     max_tokens: 4096

  # 3. Groq API 模型 (示例, 同样兼容 OpenAI)
  # llama3-70b-groq:
  #   provider: "openai_compatible"
  #   display_name: "Llama-3-70B (Groq API)"
  #   api_base: "https://api.groq.com/openai/v1"
  #   api_key: "ENV:GROQ_API_KEY" # 从环境变量读取
  #   model_name: "llama3-70b-8192"
  #   parameters:
  #     temperature: 0.6
  #     max_tokens: 8192

  # 4. 本地 Hugging Face 模型 (未来扩展方向)
  # local_hf_model:
  #   provider: "huggingface_local"
  #   display_name: "My Local HF Model"
  #   # path 将与 app_config.yaml 中的 local_models_dir 拼接
  #   path: "path/to/your/model"
  #   parameters:
  #     temperature: 0.7
  #     max_tokens: 2048

# 指令库 (System Prompts)
# 在交互式聊天中通过 /role <指令名> 来使用
instructions:
  default:
    display_name: "通用助手 (Default)"
    template: "You are a helpful and friendly AI assistant. Your responses should be clear, concise, and accurate."
  
  code_expert:
    display_name: "编程专家 (Code Expert)"
    template: "You are an expert programmer with years of experience in multiple languages, especially Python. Provide clean, efficient, and well-documented code. Always explain your reasoning and offer best practices."
  
  linux_terminal:
    display_name: "Linux终端模拟器 (Linux Terminal)"
    template: "You are a Linux terminal simulator. Emulate the behavior of a bash terminal. Respond only with the standard output of the executed command. If a command is not valid, respond with an appropriate error message (e.g., 'bash: command not found')."

  translator_en:
    display_name: "专业英译中 (Translator EN->ZH)"
    template: "You are a professional translator specializing in English to Chinese translation. Accurately translate the user's text, preserving the original tone, nuance, and meaning. Do not add any extra explanation unless asked."