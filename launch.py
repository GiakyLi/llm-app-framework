# launch.py (Restored to original format with added max_token control)
import subprocess
import time
import signal
import sys
import os
import argparse
import requests
import threading
from rich.console import Console
from llm_client.core.config_loader import ConfigLoader
from llm_client.core.exceptions import LLMAppError

def stream_output(pipe, prefix, log_file):
    """ä»å­è¿›ç¨‹çš„ç®¡é“ä¸­è¯»å–è¾“å‡ºï¼Œå†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œå¹¶æœ‰é€‰æ‹©åœ°æ‰“å°åˆ°æ§åˆ¶å°"""
    try:
        for line in iter(pipe.readline, ''):
            # åŸå§‹æ—¥å¿—è¡Œå†™å…¥æ–‡ä»¶
            if log_file:
                log_file.write(line)

            # è¿‡æ»¤INFOçº§åˆ«æ—¥å¿—
            line_upper = line.upper()
            if 'ERROR' in line_upper or 'WARNING' in line_upper:
                print(f"[{prefix}] {line}", end="")
        pipe.close()
    except Exception as e:
        print(f"Error streaming output from {prefix}: {e}")


def start_vllm_server(model_path: str, host: str, port: int, log_file, 
                        max_model_len: int = None, gpu_memory_utilization: float = 0.90):
    """åœ¨åå°å¯åŠ¨VLLMæœåŠ¡å™¨ï¼Œå¹¶å®æ—¶æ˜¾ç¤ºå…¶æ—¥å¿—"""
    command = [
        sys.executable,
        "-m", "vllm.entrypoints.openai.api_server",
        "--model", model_path,
        "--trust-remote-code",
        "--host", host,
        "--port", str(port),
        "--gpu-memory-utilization", str(gpu_memory_utilization)
    ]
    
    if max_model_len:
        command.extend(["--max-model-len", str(max_model_len)])
    
    print(f"ğŸš€ æ­£åœ¨åå°å¯åŠ¨VLLMæœåŠ¡å™¨...")
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   ç›‘å¬åœ°å€: http://{host}:{port}")
    if max_model_len:
        print(f"   æœ€å¤§æ¨¡å‹é•¿åº¦: {max_model_len} tokens")
    print(f"   æœåŠ¡å™¨æ—¥å¿—å°†ä¿å­˜åœ¨: {log_file.name}") 
    print("-" * 50)

    preexec_fn = os.setsid if sys.platform != "win32" else None
    
    server_process = subprocess.Popen(
        command,
        stdout=subprocess.PIPE,  # æ•è·æ ‡å‡†è¾“å‡º
        stderr=subprocess.PIPE,  # æ•è·æ ‡å‡†é”™è¯¯
        text=True,  # ä»¥æ–‡æœ¬æ¨¡å¼è¯»å†™ (str)
        bufsize=1,  # å¼€å¯è¡Œç¼“å†²ï¼Œç¡®ä¿æ—¥å¿—èƒ½è¢«é€è¡Œè¯»å–
        preexec_fn=preexec_fn,
        encoding='utf-8',
        errors='replace'
    )

    # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
    stdout_thread = threading.Thread(target=stream_output, args=(server_process.stdout, "VLLM-Server", log_file), daemon=True)
    stderr_thread = threading.Thread(target=stream_output, args=(server_process.stderr, "VLLM-Error", log_file), daemon=True)
    stdout_thread.start()
    stderr_thread.start()

    return server_process

def wait_for_server_ready(server_process, server_url, console: Console, timeout: int = 120):
    """ä½¿ç”¨rich.statusç­‰å¾…VLLMæœåŠ¡å™¨å‡†å¤‡å°±ç»ª"""
    with console.status("[bold yellow]â³ æ­£åœ¨ç­‰å¾…æœåŠ¡å™¨å“åº”...", spinner="dots12") as status:
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            if server_process.poll() is not None:
                return False 
                
            try:
                response = requests.get(server_url, timeout=2)
                if response.status_code == 200:
                    return True
            except requests.exceptions.RequestException:
                pass
            
            time.sleep(2)
            
    return False

def start_client(model_id: str, role_id: str):
    """åœ¨å‰å°å¯åŠ¨å®¢æˆ·ç«¯åº”ç”¨"""
    command = [
        sys.executable,
        "main.py",
        "--model", model_id,
        "--role", role_id
    ]
    print("\nğŸš€ æ­£åœ¨å¯åŠ¨å®¢æˆ·ç«¯...")
    print("-" * 50)
    # æ›¿æ¢è¿›ç¨‹ï¼Œæ›´å¥½åœ°ä¿¡å·å¤„ç†(å¦‚Ctrl+C)
    os.execv(sys.executable, command)

def main():
    console = Console()
    server_process = None
    try:
        # 1. åŠ è½½é…ç½®
        config_loader = ConfigLoader(
            app_config_path='configs/app_config.yaml',
            models_config_path='configs/models_config.yaml'
        )
        
        app_config = config_loader.app_config
        defaults = app_config.get('launcher_defaults', {})
        default_model = defaults.get('default_model', None)
        default_role = defaults.get('default_role', 'default')
        default_gpu_util = defaults.get('default_gpu_utilization', 0.70)
        default_max_len = defaults.get('default_max_model_len', None)
        
        # 2. è®¾ç½®æ—¥å¿—ç›®å½•
        log_dir = app_config.get("logging", {}).get("dir", "logs")
        os.makedirs(log_dir, exist_ok=True)  # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        vllm_log_path = os.path.join(log_dir, "vllm_server.log")

        # 3. è·å–æœåŠ¡å™¨é…ç½®
        server_config = app_config.get("vllm_server", {})
        server_host = server_config.get("host", "127.0.0.1")
        server_port = server_config.get("port", 8000)
        
        # 4. æ™ºèƒ½å¤„ç†å¥åº·æ£€æŸ¥çš„URL
        health_check_host = "127.0.0.1" if server_host == "0.0.0.0" else server_host
        server_url = f"http://{health_check_host}:{server_port}/health" # ä½¿ç”¨ /health æ¥å£
        
        # 5. è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ
        parser = argparse.ArgumentParser(description="ä¸€é”®å¯åŠ¨VLLMæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯")
        model_choices = list(config_loader.models.keys())

        if default_model and default_model in model_choices:
            final_default_model = default_model
        else:
            final_default_model = model_choices[0] if model_choices else None

        parser.add_argument("-m", "--model", type=str, default=final_default_model, choices=model_choices, help="è¦å¯åŠ¨çš„æœ¬åœ°æ¨¡å‹ID")
        parser.add_argument("-r", "--role", type=str, default=default_role, help="å®¢æˆ·ç«¯è¦ä½¿ç”¨çš„åˆå§‹è§’è‰²ID")
        parser.add_argument("--max-model-len", type=int, default=default_max_len, help="æ‰‹åŠ¨è®¾ç½®æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ä»¥é€‚åº”æ˜¾å­˜ (ä¾‹å¦‚ 8192)")
        parser.add_argument("--gpu-memory-utilization", type=float, default=default_gpu_util, help="è®¾ç½®vLLMå¯ä»¥ä½¿ç”¨çš„GPUæ˜¾å­˜æ¯”ä¾‹ (0.0 åˆ° 1.0)")
        
        args = parser.parse_args()

        if not args.model:
            console.print("[bold red]âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ä»»ä½•æ¨¡å‹ã€‚")
            sys.exit(1)

        # 6. å¯åŠ¨æµç¨‹
        with open(vllm_log_path, 'w', buffering=1, encoding='utf-8') as log_file:
            model_config = config_loader.get_model_config(args.model)
            
            is_local = "localhost" in getattr(model_config, 'api_base', '') or "127.0.0.1" in getattr(model_config, 'api_base', '')

            if not is_local:
                console.print(f"âœ… æ¨¡å‹ '{args.model}' æ˜¯ä¸€ä¸ªè¿œç¨‹APIæ¨¡å‹ï¼Œæ— éœ€å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ã€‚")
                start_client(args.model, args.role)
                return

            server_process = start_vllm_server(
                model_config.model_name, server_host, server_port, log_file,
                args.max_model_len, args.gpu_memory_utilization
            )

            # ç­‰å¾…æœåŠ¡å™¨å‡†å¤‡å°±ç»ª
            if wait_for_server_ready(server_process, server_url, console):
                console.print("[bold green]âœ… æœåŠ¡å™¨å·²å°±ç»ªï¼")
                start_client(args.model, args.role)
            else:
                console.print(f"\n[bold red]âŒ æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶æˆ–æ„å¤–é€€å‡ºï¼è¯·æ£€æŸ¥ 'logs/vllm_server.log' æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ã€‚")
                raise RuntimeError("æ— æ³•å¯åŠ¨VLLMæœåŠ¡å™¨ã€‚")

    except (Exception, KeyboardInterrupt) as e:
        if isinstance(e, KeyboardInterrupt):
            console.print("\nğŸ‘‹ æ”¶åˆ°é€€å‡ºä¿¡å·...")
        elif not isinstance(e, RuntimeError):
            console.print(f"\n[bold red]âŒ å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if server_process and server_process.poll() is None:
            console.print(f"ğŸ§¹ æ­£åœ¨å…³é—­åå°VLLMæœåŠ¡å™¨ (PID: {server_process.pid})...")
            if sys.platform != "win32":
                try:
                    os.killpg(os.getpgid(server_process.pid), signal.SIGTERM)
                except ProcessLookupError:
                    pass
            else:
                server_process.terminate()
            console.print("âœ… æ¸…ç†å®Œæˆã€‚")

if __name__ == "__main__":
    main()