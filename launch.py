# launch.py (Final Polished Version)
import subprocess
import time
import signal
import sys
import os
import argparse
import requests
import threading
from rich.console import Console
from llm_client.core.config_loader import ConfigLoader
from llm_client.core.exceptions import LLMAppError

def stream_output(pipe, prefix, log_file):
    """ä»å­è¿›ç¨‹çš„ç®¡é“ä¸­è¯»å–è¾“å‡ºï¼Œå†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œå¹¶æœ‰é€‰æ‹©åœ°æ‰“å°åˆ°æ§åˆ¶å°"""
    try:
        for line in iter(pipe.readline, ''):
            # 1. æ— è®ºå¦‚ä½•ï¼Œéƒ½å°†åŸå§‹æ—¥å¿—è¡Œå†™å…¥æ–‡ä»¶
            if log_file:
                log_file.write(line)

            # 2. [æ ¸å¿ƒä¿®æ”¹] åªæœ‰å½“æ—¥å¿—è¡ŒåŒ…å«ç‰¹å®šå…³é”®å­—æ—¶ï¼Œæ‰æ‰“å°åˆ°æ§åˆ¶å°
            #    è¿™å¯ä»¥è¿‡æ»¤æ‰ç»å¤§å¤šæ•°VLLMçš„INFOçº§åˆ«çš„å¸¸è§„æ—¥å¿—
            line_upper = line.upper()
            if 'ERROR' in line_upper or 'WARNING' in line_upper:
                print(f"[{prefix}] {line}", end="")
        pipe.close()
    except Exception as e:
        print(f"Error streaming output from {prefix}: {e}")


def start_vllm_server(model_path: str, host: str, port: int, log_file):
    """åœ¨åå°å¯åŠ¨VLLMæœåŠ¡å™¨ï¼Œå¹¶å®æ—¶æ˜¾ç¤ºå…¶æ—¥å¿—"""
    command = [
        sys.executable,
        "-m", "vllm.entrypoints.openai.api_server",
        "--model", model_path,
        "--trust-remote-code",
        "--max-model-len", "8192",
        "--host", host,
        "--port", str(port)
    ]
    
    print(f"ğŸš€ æ­£åœ¨åå°å¯åŠ¨VLLMæœåŠ¡å™¨...")
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   ç›‘å¬åœ°å€: http://{host}:{port}")
    print(f"   æœ€å¤§é•¿åº¦é™åˆ¶: 8192 tokens")
    print(f"   æœåŠ¡å™¨æ—¥å¿—å°†ä¿å­˜åœ¨: {log_file.name}") # ä½¿ç”¨ log_file.name è·å–è·¯å¾„
    print("-" * 50)

    preexec_fn = os.setsid if sys.platform != "win32" else None
    
    server_process = subprocess.Popen(
        command,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        bufsize=1,
        preexec_fn=preexec_fn
    )

    # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹ï¼Œå°†æ—¥å¿—æ–‡ä»¶å¥æŸ„ä¼ é€’è¿›å»
    stdout_thread = threading.Thread(
        target=stream_output, 
        args=(server_process.stdout, "VLLM-Server", log_file)
    )
    stderr_thread = threading.Thread(
        target=stream_output, 
        args=(server_process.stderr, "VLLM-Error", log_file)
    )
    stdout_thread.daemon = True
    stderr_thread.daemon = True
    stdout_thread.start()
    stderr_thread.start()

    return server_process

def wait_for_server_ready(server_process, server_url, console: Console, timeout: int = 60):
    """ä½¿ç”¨rich.statusç­‰å¾…VLLMæœåŠ¡å™¨å‡†å¤‡å°±ç»ª"""
    
    # ä½¿ç”¨rich.statusåˆ›å»ºä¸€ä¸ªåŠ¨æ€çš„åŠ è½½åŠ¨ç”»
    with console.status("[bold yellow]â³ æ­£åœ¨ç­‰å¾…æœåŠ¡å™¨å“åº”...", spinner="dots12") as status:
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            # æ£€æŸ¥æœåŠ¡å™¨è¿›ç¨‹æ˜¯å¦å·²ç»æ„å¤–é€€å‡º
            if server_process.poll() is not None:
                return False # å¦‚æœè¿›ç¨‹é€€å‡ºï¼Œç›´æ¥è¿”å›å¤±è´¥
                
            try:
                response = requests.get(server_url, timeout=2) # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶è¿›è¡Œè½®è¯¢
                if response.status_code == 200:
                    return True # æœåŠ¡å™¨å°±ç»ªï¼Œè¿”å›æˆåŠŸ
            except requests.exceptions.RequestException:
                pass # å¿½ç•¥è¿æ¥é”™è¯¯ï¼Œç»§ç»­ç­‰å¾…
            
            time.sleep(2)
            
    return False # å¦‚æœå¾ªç¯ç»“æŸä»æœªå°±ç»ªï¼Œåˆ™ä¸ºè¶…æ—¶

def start_client(model_id: str, role_id: str):
    """åœ¨å‰å°å¯åŠ¨å®¢æˆ·ç«¯åº”ç”¨"""
    command = [
        sys.executable,
        "main.py",
        "--model", model_id,
        "--role", role_id
    ]
    print("\nğŸš€ æ­£åœ¨å¯åŠ¨å®¢æˆ·ç«¯...")
    print("-" * 50)
    subprocess.run(command)

def main():
    # [æ ¸å¿ƒä¿®æ”¹] åˆ›å»ºä¸€ä¸ªConsoleå¯¹è±¡ä¾›åç»­ä½¿ç”¨
    console = Console()

    try:
        config_loader = ConfigLoader(
            app_config_path='configs/app_config.yaml',
            models_config_path='configs/models_config.yaml'
        )
        
        app_config = config_loader.app_config
        
        log_config = app_config.get("logging", {})
        log_dir = log_config.get("dir", "logs")
        os.makedirs(log_dir, exist_ok=True)
        vllm_log_path = os.path.join(log_dir, "vllm_server.log")

        server_config = app_config.get("vllm_server", {})
        server_host = server_config.get("host", "127.0.0.1")
        server_port = server_config.get("port", 8000)
        server_url = f"http://127.0.0.1:{server_port}/v1/models"
        
        model_choices = list(config_loader.models.keys())
        parser = argparse.ArgumentParser(description="ä¸€é”®å¯åŠ¨VLLMæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯")
        parser.add_argument("-m", "--model", type=str, default=model_choices[0] if model_choices else None, choices=model_choices, help="è¦å¯åŠ¨çš„æœ¬åœ°æ¨¡å‹ID")
        parser.add_argument("-r", "--role", type=str, default="default", help="å®¢æˆ·ç«¯è¦ä½¿ç”¨çš„åˆå§‹è§’è‰²ID")
        args = parser.parse_args()

        if not args.model:
            console.print("[bold red]âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ä»»ä½•æ¨¡å‹ã€‚")
            sys.exit(1)

        server_process = None
        
        with open(vllm_log_path, 'w', buffering=1, encoding='utf-8') as log_file:
            model_config = config_loader.get_model_config(args.model)
            
            should_start_server = "localhost" in getattr(model_config, 'api_base', '') or \
                                "127.0.0.1" in getattr(model_config, 'api_base', '')

            if not should_start_server:
                console.print(f"âœ… æ¨¡å‹ '{args.model}' æ˜¯ä¸€ä¸ªè¿œç¨‹APIæ¨¡å‹ï¼Œæ— éœ€å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ã€‚")
                start_client(args.model, args.role)
                return

            server_process = start_vllm_server(model_config.model_name, server_host, server_port, log_file)

            # [æ ¸å¿ƒä¿®æ”¹] å°†consoleå¯¹è±¡ä¼ é€’è¿›å»ï¼Œå¹¶æ ¹æ®è¿”å›ç»“æœæ‰“å°æœ€ç»ˆçŠ¶æ€
            if wait_for_server_ready(server_process, server_url, console):
                console.print("[bold green]âœ… æœåŠ¡å™¨å·²å°±ç»ªï¼")
                start_client(args.model, args.role)
            else:
                console.print(f"\n[bold red]âŒ æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶æˆ–æ„å¤–é€€å‡ºï¼è¯·æ£€æŸ¥ 'logs/vllm_server.log' æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ã€‚")
                raise RuntimeError("æ— æ³•å¯åŠ¨VLLMæœåŠ¡å™¨ã€‚")

    except (Exception, KeyboardInterrupt) as e:
        if isinstance(e, KeyboardInterrupt):
            console.print("\nğŸ‘‹ æ”¶åˆ°é€€å‡ºä¿¡å·...")
        elif not isinstance(e, RuntimeError):
            console.print(f"\n[bold red]âŒ å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if server_process and server_process.poll() is None:
            console.print(f"ğŸ§¹ æ­£åœ¨å…³é—­åå°VLLMæœåŠ¡å™¨ (PID: {server_process.pid})...")
            if sys.platform != "win32":
                os.killpg(os.getpgid(server_process.pid), signal.SIGTERM)
            else:
                server_process.terminate()
            console.print("âœ… æ¸…ç†å®Œæˆã€‚")

if __name__ == "__main__":
    main()